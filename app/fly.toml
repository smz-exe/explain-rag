# fly.toml app configuration file generated for explainrag-api on 2026-02-26T03:28:28+09:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'explainrag-api'
primary_region = 'nrt'

[build]
dockerfile = 'Dockerfile'

[env]
API_HOST = '0.0.0.0'
API_PORT = '8000'
ENVIRONMENT = 'production'
# First deploy: models download to volume. After cached, set to 'true' for strict offline mode
HF_OFFLINE_MODE = 'false'
PRELOAD_MODELS = 'true'

[http_service]
internal_port = 8000
force_https = true
auto_stop_machines = 'off'
auto_start_machines = true
min_machines_running = 1
processes = ['app']

[http_service.concurrency]
type = 'requests'
hard_limit = 100
soft_limit = 80

[[http_service.checks]]
interval = '30s'
timeout = '10s'
grace_period = '1m0s'
method = 'GET'
path = '/health'
protocol = 'http'

# Persistent volume for ML models (survives deploys)
[mounts]
source = 'models_cache'
destination = '/app/models'

[[vm]]
memory = '4gb'
cpu_kind = 'shared'
cpus = 2
